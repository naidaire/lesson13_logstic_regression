{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Logistic Regression\n",
    " \n",
    "_Authors: Multiple_\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to..\n",
    "\n",
    "- Explain how logistic regression modifies linear regression for classification problems.\n",
    "- Interpret logistic regression coefficients.\n",
    "- Build a logistic regression model.\n",
    "- Calculate accuracy, true positive rate, and false negative rate from a confusion matrix.\n",
    "- Explain the limitations of accuracy as a classification metric.\n",
    "- Explain how to trade off true positive and false negative rate in a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Tonight we are talking about logistic regression, which despite its name is a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "\n",
    "/poll “Which of the following are classification (as opposed to regression) problems? (Select all that apply.)” “Predicting how many people will come to a meetup event.” “Predicting which of the people who signed up for a meetup will actually attend.” “Predicting the price that a house will sell for, based on its zip code and square footage.” “Assigning probabilities of experiencing a fire in the next six months to buildings in a city.” “Identifying animals in photographs by species.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refresher-fitting-and-visualizing-a-linear-regression-using-scikit-learn\"></a>\n",
    "## Refresher: Fitting and Visualizing a Linear Regression Using scikit-learn\n",
    "---\n",
    "\n",
    "Use Pandas to load in the glass attribute data from the UCI machine learning website. The columns are different measurements of properties of glass that can be used to identify the glass type. For detailed information on the columns in this data set, [please see the included .names file](http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_filepath = Path('.', 'data', 'glass.csv')\n",
    "glass = pd.read_csv(glass_filepath)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change columns to something more uniform\n",
    "glass.columns = ['ri','na','mg','al','si','k','ca','ba','fe','glass_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Dictionary**\n",
    "\n",
    "- `Id`: number: 1 to 214\n",
    "- `RI`: refractive index  \n",
    "- `Na`: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n",
    "- `Mg`: Magnesium\n",
    "- `Al`: Aluminum\n",
    "- `Si`: Silicon\n",
    "- `K` : Potassium\n",
    "- `Ca`: Calcium\n",
    "- `Ba`: Barium\n",
    "- `Fe`: Iron\n",
    "- `Type` : Type of glass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's build a regression model for refractice index against aluminum content.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x11520ecc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAFPCAYAAADNzUzyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11520ee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scatter with regression line\n",
    "sns.lmplot(data=glass, x='al', y='ri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a linear regression model (name the model \"linreg\").\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "feature_cols = ['al']\n",
    "X = glass.loc[:, feature_cols]\n",
    "y = glass.loc[:, 'ri']\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for all values of X and add back to the original DataFrame.\n",
    "\n",
    "glass.loc[:, 'predictions'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the predicted `ri` against each `al` as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8646f1628f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot those ri against al - connected by a line (try plt.plot()).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'al'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.plot(glass.loc[:,'al'], glass.loc[:,'predictions'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2628\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1867\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1678\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'label'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m                 \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Don't modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m                 \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prediction'"
     ]
    }
   ],
   "source": [
    "# Plot those ri against al - connected by a line (try plt.plot()).\n",
    "\n",
    "glass.plot(kind='line', x='al', y='prediction')\n",
    "\n",
    "#plt.plot(glass.loc[:,'al'], glass.loc[:,'predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note the y axis labels when comparing to the scatterplot above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot this regression line with the scatter points on the same chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the plots together (use a scatter and line graph).\n",
    "#use plt to plot bot the predicted line, and the scatter plot of al vs ri\n",
    "\n",
    "plt.plot(glass.loc[:,'al'], glass.loc[:, 'predictions'], color='red')\n",
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'ri']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How good would you say that this model is, based on the graph? Suggestion: think about how it compares to a \"null model\" that just predicts the mean reflective index regardless of the aluminum content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "null_pred = np.ones(y.shape)\n",
    "null_pred *= y.mean()\n",
    "\n",
    "null_rmse = (mean_squared_error(y, null_pred))**(0.5)\n",
    "pred_rmse = (mean_squared_error(y, glass.loc[:, 'predictions']))**(0.5)\n",
    "\n",
    "print(null_rmse)\n",
    "print(pred_rmse)\n",
    "print((pred_rmse - null_rmse) / null_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print out the intercept and coefficient values from our fit `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do these numbers mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manually compute the predicted value of `ri` when `al=2.0` using the regression equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction for al=2 using the equation.\n",
    "1.52194533024 + 2*(-0.00247761)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction for al=2 using the predict method.\n",
    "linreg.predict(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predicting-a-categorical-response\"></a>\n",
    "## Predicting a Single Categorical Response\n",
    "---\n",
    "\n",
    "Linear regression is appropriate when we want to predict the value of a continuous target/response variable, but what about when we want to predict membership in a class or category?\n",
    "\n",
    "**Examine the glass type column in the data set. What are the counts in each category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine glass_type.\n",
    "glass.loc[:, 'glass_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say these types are subdivisions of broader glass types:\n",
    "\n",
    "> **Window glass:** types 1, 2, and 3\n",
    "\n",
    "> **Household glass:** types 5, 6, and 7\n",
    "\n",
    "**Create a new `household` column that indicates whether or not a row is household glass, coded as 1 or 0, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression can only answer Yes/No Questions\n",
    "# Types 1, 2, 3 are window glass.\n",
    "# Types 5, 6, 7 are household glass.\n",
    "# 1 for yes, 0 for no\n",
    "\n",
    "glass.loc[:, 'household'] = glass.loc[:, 'glass_type'].map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.loc[:, 'household'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our task, so that we're predicting the `household` category using `al`. Let's visualize the relationship to figure out how to do this.\n",
    "\n",
    "**Make a scatter plot comparing `al` and `household`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'household'])\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household');\n",
    "\n",
    "#we can see that there's higher 'al' in household glass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a new `LinearRegression` predicting `household` from `al`.**\n",
    "\n",
    "Let's draw a regression line like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression model and store the predictions.  \n",
    "# SORT OF THE WRONG WAY OF DOING THIS\n",
    "\n",
    "feature_cols = ['al']\n",
    "X = glass.loc[:, feature_cols] \n",
    "y = glass.loc[:, 'household']\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X,y)\n",
    "\n",
    "glass.loc[:, 'household_pred'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot that includes the regression line\n",
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'household'], alpha=.2)\n",
    "plt.plot(glass.loc[:, 'al'], glass.loc[:, 'household_pred'], color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **al=3**, what class do we predict for household? **1**\n",
    "\n",
    "If **al=1.5**, what class do we predict for household? **0**\n",
    "\n",
    "We predict the 0 class for **lower** values of al, and the 1 class for **higher** values of al. What's our cutoff value? Around **al=2**, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n",
    "\n",
    "Therefore, we'll say that if **household_pred >= 0.5**, we predict a class of **1**, else we predict a class of **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using this threshold, create a new column of our predictions for whether a row is household glass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where returns the first value if the condition is True,\n",
    "# and the second value if the condition is False.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nums = np.array([5, 15, 8])\n",
    "\n",
    "np.where(nums > 10, 'BIG', 'small')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform household_pred to 1 or 0.\n",
    "glass.loc[:, 'household_pred_class'] = np.where(glass.loc[:, 'household_pred']>= 0.5, 1, 0)\n",
    "glass.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot a line that shows our predictions for class membership in household vs. not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.sort_values('al', inplace=True)\n",
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'household'])\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')\n",
    "\n",
    "glass.sort_values('al', inplace=True)\n",
    "plt.plot(glass.loc[:, 'al'], glass.loc[:, 'household_pred_class'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression yields a reasonable binary classifier in this case when we map values above 0.5 to 1 and values below 0.5 to 0.\n",
    "\n",
    "It would be nice if we could also interpret the raw numbers it gives us, e.g. as probabilities. The problem is that linear regression is unbounded. As a result, it gives values below 0 and above 1, which cannot be probabilities.\n",
    "\n",
    "This is where logistic regression comes in: it basically takes that linear regression line and bends its ends into an S-shape so that it always stays between 0 and 1, so that we can interpret its outputs as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-logistic-regression-for-classification\"></a>\n",
    "## Using Logistic Regression for Classification\n",
    "---\n",
    "\n",
    "**Import the `LogisticRegression` class from `linear_model` below and fit the same regression model predicting `household` from `al`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a logistic regression model and store the class predictions.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "feature_cols = ['al']\n",
    "X = glass.loc[:, feature_cols]\n",
    "y = glass.loc[:, 'household']\n",
    "\n",
    "logreg.fit(X,y)\n",
    "glass.loc[:, 'household_pred_logreg'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted class using the logistic regression as we did for the linear regression predictions above.**\n",
    "\n",
    "As you can see, the class predictions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the class predictions.\n",
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'household'])\n",
    "plt.plot(glass.loc[:, 'al'], glass.loc[:, 'household_pred_logreg'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted the predicted probabilities instead of just the class predictions, to understand how confident we are in a given prediction?\n",
    "\n",
    "**Using the built-in `.predict_proba()` function, examine the predicted probabilities for the first handful of rows of `X`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities using .predict_proba()\n",
    "# returns 2 columns\n",
    "# probability assigned to 0 vs 1\n",
    "# windows vs household glass\n",
    "\n",
    "logreg.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn orders the columns according to our class labels. The two-column output of `predict_proba` returns a column for each class of our `household` variable. The first column is the probability of `household=0` for a given row, and the second column is the probability of `household=1`.\n",
    "\n",
    "**Store the predicted probabilities of class=1 in its own column in the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the predicted probabilities of class 1.\n",
    "\n",
    "glass.loc[:, 'household_pred_prob'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the predicted probabilities as a line on our plot (probability of `household=1` as `al` changes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted probabilities.  You get the nice S shape \n",
    "\n",
    "plt.scatter(glass.loc[:, 'al'], glass.loc[:, 'household'])\n",
    "plt.plot(glass.loc[:, 'al'], glass.loc[:, 'household_pred_prob'], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine some example predictions.\n",
    "print(logreg.predict_proba(1))\n",
    "\n",
    "#89% that it's 0 = Window, not household\n",
    "\n",
    "print(logreg.predict_proba(2))\n",
    "print(logreg.predict_proba(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- Build a logistic regression model for `household` using two features of your choice.\n",
    "- Do a simple train-test split on `glass`.\n",
    "- Train your model on the training set and evaluate it with `model.score` on the test set.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Try out different sets of features to see which give the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "feature_cols = ['na', 'mg']\n",
    "X = glass.loc[:, feature_cols]\n",
    "y = glass.loc[:, 'household']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred_logpred_red_namg = logreg.predict(X_test)\n",
    "#glass.loc[:, 'household_pred_logreg'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logpred_red_namg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model score\n",
    "print(metrics.accuracy_score(y_test, y_pred_logpred_red_namg))\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probability-odds-ratio-e-log-and-log-odds\"></a>\n",
    "## Understanding Logistic Regression\n",
    "---\n",
    "\n",
    "**Recall:** A coefficient in a *linear regression* model tells you how the *number* predicted by the model changes when the associated variable increases by one and all other variables remain the same.\n",
    "\n",
    "**Similarly**, A coefficient in a *logistic regression* model tells you how the *log odds* predicted by the model changes when the associated variable increases by one and all other variables remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to develop some intuitions about log odds to help us reason about our logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
    "\n",
    "It is often useful to think of the numeric odds as a ratio. For example, 5/1 = 5 odds is \"5 to 1\" -- five wins for every one loss (e.g. of six total plays). 2/3 odds means \"2 to 3\" -- two wins for every three losses (e.g. of five total plays).\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
    "\n",
    "$$odds = \\frac {probability} {1 - probability}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an example we can create a table of probabilities vs. odds, as seen below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a table of probability versus odds.\n",
    "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\n",
    "table['odds'] = table.probability / (1 - table.probability)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "\n",
    "Convert the following probabilities to odds:\n",
    "\n",
    "1. .25\n",
    "1. 1/3\n",
    "1. 2/3\n",
    "1. .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'probability':[0.25, 1/3, 2/3, .95]})\n",
    "table['odds'] = table.probability / (1 - table.probability)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"understanding-e-and-the-natural-logarithm\"></a>\n",
    "### Understanding the Natural Logarithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logarithm tells you the *order of magnitude* of a number. The base-10 logarithm is a continuous version of \"the number of times you would need to multiply 10 to get that number.\"\n",
    "\n",
    "| number | number as a power of 10 | $\\log_{10}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $1 $|$ 10^0$ | 0 |\n",
    "| $10 $|$ 10^1$ | 1 |\n",
    "| $100 $|$ 10^2$ | 2 |\n",
    "| $1000 $|$ 10^3$ | 3 |\n",
    "\n",
    "It also works in the other direction:\n",
    "\n",
    "| number | number as a power of 10 | $\\log_{10}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $.001 $ | $ 10^{-3}$ | -3 |\n",
    "| $.01 $ | $ 10^{-2}$ | -2 |\n",
    "| $.1 $|$ 10^{-1}$ | -1 |\n",
    "| $1 $|$ 10^0$ | 0 |\n",
    "\n",
    "And for numbers in between exact powers of 10:\n",
    "\n",
    "| number | number as a power of 10 | $\\log_{10}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $1$ | $ 10^{0}$ | 0 |\n",
    "| $2$ | $ 10^{.301}$ | .301 |\n",
    "| $5$|$ 10^{.699}$ | .699 |\n",
    "| $10$|$ 10^1$ | 1 |\n",
    "| $20$|$ 10^{1.301}$ | 1.301 |\n",
    "| $50$|$ 10^{1.699}$ | 1.699 |\n",
    "| $100$|$ 10^2$ | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base $e$.** It is often convenient to use the special number $e$ as a base instead of 10. The interpretation is analogous: the base-$e$ logarithm of a number is a continuous version of \"the number of times you would have to multiple $e$ to get that number.\"\n",
    "\n",
    "For instance:\n",
    "\n",
    "| number | number as a power of $e$ | $\\log_{e}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $1 $|$ e^0$ | 0 |\n",
    "| $2.718$|$ e^1$ | 1 |\n",
    "| $7.39$|$ e^2$ | 2 |\n",
    "| $20.09$|$ e^3$ | 3 |\n",
    "\n",
    "It also works in the other direction:\n",
    "\n",
    "| number | number as a power of $e$ | $\\log_{e}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $.050 $ | $ e^{-3}$ | -3 |\n",
    "| $.135 $ | $ e^{-2}$ | -2 |\n",
    "| $.368 $|$ e^{-1}$ | -1 |\n",
    "| $1 $|$ e^0$ | 0 |\n",
    "\n",
    "And for numbers in between exact powers of $e$:\n",
    "\n",
    "| number | number as a power of $e$ | $\\log_{e}$(number) |\n",
    "| ------ | ----------------------------- |\n",
    "| $1$ | $ e^{0}$ | 0 |\n",
    "| $1.35$ | $ e^{.301}$ | .301 |\n",
    "| $2.01$|$ e^{.699}$ | .699 |\n",
    "| $2.718$|$ e^1$ | 1 |\n",
    "| $3.67$|$ e^{1.301}$ | 1.301 |\n",
    "| $5.47$|$ e^{1.699}$ | 1.699 |\n",
    "| $7.39$|$ e^2$ | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take the **logarithm** of an **odds** we get the **log odds**.\n",
    "\n",
    "The most common convention is to use base-$e$ logarithms unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log odds to the table.\n",
    "table['logodds'] = np.log(table['odds'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** log odds goes to $-\\infty$ as probability goes to 0, and goes to $\\infty$ as probability goes to 1.\n",
    "\n",
    "**Consequence:** The fact that linear model is unbounded is fine if we use it to model *log odds* rather than *probability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-logistic-regression\"></a>\n",
    "### What Is Logistic Regression?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression:** *Continuous response* is modeled as a linear combination of the features.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "**Logistic regression:** *Log odds* is modeled as a linear combination of the features.\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation can be rearranged to get the predicted probability:\n",
    "\n",
    "$$\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$\n",
    "\n",
    "This equation gives us the \"S\" (sigmoid) shape for the predicted probability as a function of $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we interpret the regression parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression:**\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "- $\\beta_0$ tells you the model's prediction for $y$ when all input features are zero.\n",
    "- $\\beta_1$ tells you how the model's prediction for $y$ changes with a one-unit increase in $x$ when all other variables remain the same.\n",
    "\n",
    "**Logistic regression:**\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "- $\\beta_0$ tells you the model's prediction for the *log odds of $y$* when all input features are zero.\n",
    "- $\\beta_1$ tells you how the model's prediction for *the log odds of* $y$ changes with a one-unit increase in $x$ when all other variables remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1.3149 coefficient on na means that predicted log odds that the glass is household decrease by 1.31, when na content increases by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottom line:** A positive coefficient means that the predicted log odds of the response (and thus the predicted probability) increases with the associated variable, while a negative coefficient means that it decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logistic regression beta values](./assets/logistic_betas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the $\\beta_0$ value shifts the curve horizontally, whereas changing the $\\beta_1$ value changes the slope of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Logistic regression addresses a binary classification problem by modeling the *log odds* that an individual is in the class as a linear function of the model features.\n",
    "- A coefficient in a logistic regression model tells you *how the log odds that the model predicts changes* with a one-unit increase in the associated input feature, while other features remain unchanged.\n",
    "- The model's log-odds predictions can be transformed into *probabilities*.\n",
    "- Those predicted probabilities follow an \"s\" (sigmoid) shape that is bounded by 0 and 1, as a function of the input features.\n",
    "- Those predicted probabilities can be converted into \"hard\" class predictions by mapping everything above a threshold to 1 and everything below it to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-logistic-regression-to-other-models\"></a>\n",
    "## Comparing Logistic Regression to Other Models\n",
    "---\n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Somewhat interpretable.\n",
    "- Training and prediction are fast.\n",
    "- Outputs probabilities.\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log odds of the response.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-classification-metrics\"></a>\n",
    "## Advanced Classification Metrics\n",
    "\n",
    "---\n",
    "\n",
    "By default, the `.score` method of a logistic regression model in sklearn returns accuracy:\n",
    "\n",
    "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n",
    "\n",
    "However, accuracy is not always the most relevant metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the **confusion matrix** for a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\"><font color=\"blue\">TN = 50</font></td>\n",
    "    <td style=\"text-align: center\"><font color=\"red\">FP = 10</font></td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\"><font color=\"orange\">FN = 5</font></td>\n",
    "    <td style=\"text-align: center\"><font color=\"green\">TP = 100</font></td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "- <font color=\"green\">**True positives (TP):**</font> These are cases in which we predicted yes (smokers), and they actually are smokers.\n",
    "- <font color=\"blue\">**True negatives (TN):**</font> We predicted no, and they are nonsmokers.\n",
    "- <font color=\"red\">**False positives (FP):**</font> We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n",
    "- <font color=\"orange\">**False negatives (FN):**</font> We predicted no, but they are smokers. (This is also known as a \"Type II error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "\n",
    "Categorize these cases as TP, TN, FP, or FN.\n",
    "    \n",
    "- We predict that a growth is malignant, and it is benign. (is_malignant=1) - False Negative (False Positive)\n",
    "- We predict that an image does not contain a cat, and it does not. (has_cat=1) True Negative\n",
    "- We predict that a locomotive will fail in the next two weeks, and it does. (breaks=1)  True Positive\n",
    "- We predict that a user will like a song, and she does not. (likes_song=1)  False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"accuracy-true-positive-rate-and-false-negative-rate\"></a>\n",
    "### Accuracy, True Positive Rate, and False Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** Overall, how often is the classifier correct?\n",
    "\n",
    "<span>\n",
    "    (<span style=\"color: green\">TP</span>+<span style=\"color: blue\">TN</span>)/<span style=\"color: purple\">total</span> = (<span style=\"color: green\">100</span>+<span style=\"color: blue\">50</span>)/<span style=\"color: purple\">165</span> = 0.91\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom; color: purple\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center; background-color: blue\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center; background-color: green\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True positive rate (TPR)** asks, “Out of all of the target class labels, how many were accurately predicted to belong to that class?”\n",
    "\n",
    "For example, given a medical exam that tests for cancer, how often does it correctly identify patients with cancer?\n",
    "\n",
    "<span>\n",
    "<span style=\"color: green\">TP</span>/<span style=\"color: aqua\">actual yes</span> = <span style=\"color: green\">100</span>/<span style=\"color: aqua\">105</span> = 0.95\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center;background-color: green\">TP = 100</td>\n",
    "    <td style=\"text-align: center;color: aqua\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False positive rate (FPR)** asks, “Out of all items not belonging to a class label, how many were predicted as belonging to that target class label?”\n",
    "\n",
    "For example, given a medical exam that tests for cancer, how often does it trigger a “false alarm” by incorrectly saying a patient has cancer?\n",
    "\n",
    "<span>\n",
    "<span style=\"color: orange\">FP</span>/<span style=\"color: fuchsia\">actual no</span> = <span style=\"color: orange\">10</span>/<span style=\"color: fuchsia\">60</span> = 0.17\n",
    "</span>\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center;background-color: orange\">FP = 10</td>\n",
    "    <td style=\"text-align: center;color:fuchsia\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "\n",
    "You can tune your model to avoid Type 1 or Type 2 errors\n",
    "\n",
    "We turn the probabilities output by a logistic regression model into \"hard\" predictions by setting a threshold. For instance, we might treat all probabilities above .5 as positive predictions and the rest as negative predictions.\n",
    "\n",
    "- How does the true positive rate of a logistic regression model change if we change the threshold probability for treating a prediction as positive from .5 to .6?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer:  Dec True Positive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How does the false positive rate of a logistic regression model change if we change the threshold probability for treating a prediction as positive from .5 to .6?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer: Dec. False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe a situation in which you would want to use a high threshold probability."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "answer:  Criminal Justice.  You want to be very confident that the person is guilty.  better to let a guilty person free, than to convict an innocent person\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe a situation in which you would want to use a low threshold probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  Security Screening\n",
    "Pregnancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the accuracy, true positive rate, and false positive rate for the confusion matrix below.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 140</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">30</td>\n",
    "    <td style=\"text-align: center\">10</td>\n",
    "    <td style=\"text-align: center\">40</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "    <td style=\"text-align: center\">40</td>\n",
    "    <td style=\"text-align: center\">100</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">90</td>\n",
    "    <td style=\"text-align: center\">50</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "accuracy = (30+40)/140 = 70/140 = .50\n",
    "\n",
    "True Positive Rate = 40/100 = .40\n",
    "\n",
    "False Positive Rate = 10/40 = .25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics, model_selection\n",
    "\n",
    "admissions_path = Path('.', 'data', 'admissions.csv')\n",
    "admissions = pd.read_csv(admissions_path).dropna()\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data, train model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "feature_cols = ['gre']\n",
    "X = admissions.loc[:, feature_cols]\n",
    "y = admissions.loc[:, 'admit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 46)\n",
    "logit_simple = LogisticRegression()\n",
    "logit_simple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64000000000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model - our model gets it right 64% of the time\n",
    "logit_simple.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64000000000000001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Compare to null model\n",
    "# chose 0 zero because our null model says no everytime\n",
    "# y.mean() - 32% people were admitted, so more likely NOT ADMITTED\n",
    "y_pred_null = np.zeros(y_test.shape)\n",
    "y_pred_null\n",
    "\n",
    "# meaning - There's no predictive power to just saying No Everytime\n",
    "metrics.accuracy_score(y_test, y_pred_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64,  0],\n",
       "       [36,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "logit_pred_proba = logit_simple.predict_proba(X_test)[:, 1]\n",
    "y_pred = logit_pred_proba > .5\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- What is our model doing?\n",
    "- What is the model's accuracy on the test set?\n",
    "- What is the model's true positive rate?\n",
    "- What is the model's false positive rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Predicting No everytime\n",
    "\n",
    "Accuracy: 64%\n",
    "\n",
    "True Positive Rate: 0%\n",
    "\n",
    "False Positive Rate: 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can vary the classification threshold for our model to get different predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 39],\n",
       "       [ 9, 27]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logit_pred_proba > .3\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "- What is the model's accuracy on the test set?\n",
    "- What is the model's true positive rate?\n",
    "- What is the model's false positive rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 25+27/(100) = 52%\n",
    "\n",
    "TPR: 27/(27+9) = 75%\n",
    "\n",
    "FPR: 39/(39+25) = 61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Intuitive: it's a lot like an exam score where you get total correct/total attempted.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Potentially misleading: Can look OK when model is just outputting the most common label.\n",
    "    - Particularly bad when classes are imbalanced -- e.g. train doesn't break 99% of the time, so a model that always says \"won't break\" has 99% accuracy -- but it fails exactly when we need it!\n",
    "- Doesn't account for relative costs of false positives and false negatives.\n",
    "- Doesn't say anything about how far predicted probabilities are from the correct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other metrics to investigate:  All avail in scikitlearn**\n",
    "    \n",
    "- **Classification error:** Proportion of incorrect predictions (1-accuracy, lower is better).\n",
    "- **Receiver Operating Characteristic (ROC) curves:** True positive rate vs. false positive rate across all possible threshold probabilities. The **area under the ROC curve** (AUC) is a measure of how well your model performs overall across those thresholds.\n",
    "  - Allows you to visualize the performance of your classifier across all possible classification thresholds, thus helping you to choose a threshold that appropriately balances true positives and false positives.\n",
    "  - Still useful when there is high class imbalance (unlike classification accuracy/error).\n",
    "  - Harder to use when there are more than two response classes.\n",
    "- **Log loss**: Measures how far the output probabilities are from the correct labels. (Useful when you want to make expected value calculations with those probabilities or triage cases for further attention.)\n",
    "- **True Negative Rate**, **False Negative Rate**\n",
    "- **Recall** (a.k.a. True Positive Rate), **Precision** (proportion of positive predictions that are true)\n",
    "\n",
    "These measures are all readily available in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Review\n",
    "- **Logistic regression**\n",
    "  - What kind of machine learning problems does logistic regression address?\n",
    "  - What do the coefficients in a logistic regression represent? How does the interpretation differ from ordinary least squares? How is it similar?\n",
    "  \n",
    "- **The confusion matrix**\n",
    "  - Why isn't accuracy all you need to evaluate classification models?\n",
    "  - How can you tune a model based on the relative costs of false positives and false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    "\n",
    "- Final Project Pt 2 due today\n",
    "- Final Project Pt 3 due Thurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=========================================\n",
    "@channel\n",
    "Exit Ticket: https://goo.gl/forms/OUw4gyTiRKMOTI3t2        \n",
    "\n",
    "#feedback\n",
    "=========================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random sample of pandas dataframe in case you need to parse down the data\n",
    "\n",
    "help(glass.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instarcart_data = instacart_data.sample(frac=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
